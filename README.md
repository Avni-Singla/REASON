# REASON ğŸ”
Image-based REliability & SOuRce iNspectioN for Deep Learning Models
## ğŸ“Œ Overview
REASON is a reliability-aware, introspective framework designed to predict whether a deep learning modelâ€™s prediction is correct, rather than relying on poorly calibrated softmax confidence scores. It operates as a post-hoc, architecture-agnostic reliability estimator that enhances trust in image classification systems.
The framework augments a ResNet50 backbone with a lightweight meta-model (MetaNet) that analyzes internal signals of the base model to estimate prediction correctness.

## ğŸ¯ Key Contributions
- Predicts correctness probability, not just class confidence
- Mitigates overconfident wrong predictions
- Enables selective prediction through safe rejection
- Uses introspective meta-features extracted from internal model behavior
- Improves trust and safety in deployment-critical AI systems

## ğŸ§  Core Idea
Softmax confidence is often miscalibrated and unreliable, especially for ambiguous or out-of-distribution inputs.
REASON addresses this by learning a secondary correctness predictor using enriched signals from inside the network.

## ğŸ“‚ Dataset
REASON is evaluated using the **Imagenette** dataset, a curated subset of ImageNet designed for rapid experimentation while preserving ImageNet semantics.

ğŸ”— **Imagenette Dataset:**  
https://github.com/fastai/imagenette


## ğŸ§© Meta-Features Used
REASON constructs a high-dimensional reliability vector using:
- Entropy of class probabilities
- Logit gap between top predictions
- Top-k logits
- Monte Carlo dropout variance
- Gradient sensitivity (input-level fragility)
- PCA-compressed penultimate embeddings
- K-Nearest Neighbor distances in feature space
These signals are combined and fed into a MetaNet (MLP) to estimate correctness probability.

## âš™ï¸ Architecture
```
Input Image
     â”‚
ResNet50 Backbone
     â”‚
Internal Signals (logits, features, gradients)
     â”‚
Meta-Feature Construction
     â”‚
MetaNet (Correctness Predictor)
     â”‚
Prediction + Reliability Score
```

## ğŸ“‚ Repository Structure
```
REASON/
â”‚
â”œâ”€â”€ index.html              # Interactive frontend demo
â”œâ”€â”€ reason_api.py           # Flask backend for inference
â”œâ”€â”€ reason_saved/           # Trained models, PCA, KNN, calibrators
â”œâ”€â”€ eval_plots/             # ROC curves, diagnostics, visualizations
â”œâ”€â”€ README.md               # Project documentation
```

## ğŸš€ How to Run (Local / Demo)
### 1. Install dependencies
```bash
pip install torch torchvision flask flask-cors numpy joblib pillow matplotlib
```

### 2. Place model artifacts
Ensure the folder reason_saved/ contains:
- best_base.pth
- best_meta_enhanced.pth
- penultimate_pca.pkl
- knn_bank.pkl
- dataset_meta.json

### 3. Start backend
```bash
python reason_api.py
```
Backend runs at:
```
http://127.0.0.1:5000
```

### 4. Open demo UI
Open index.html in your browser and upload an image.

## ğŸ“Š Results Summary
- Correctness Prediction AUC: 0.817
- Average Precision (AP): 0.954
- Effectively detects high-confidence incorrect predictions
- Improves trusted accuracy via selective rejection

## ğŸ” Explainability & Diagnostics
- Class probability bar charts
- Meta-vector visualization (first 60 dimensions)
- Gradient sensitivity and uncertainty diagnostics
- Raw JSON outputs for debugging and analysis

## ğŸ§ª Generalization
REASON demonstrates strong robustness on:
- Unseen real-world images
- Out-of-distribution samples
- Ambiguous inputs with misleading softmax confidence

## ğŸ“„ Research Paper
REASON: A Reliability-Aware Introspective Image Classification Framework Using Enhanced Meta-Feature Modeling
- Accepted at IEEE International Conference on Sustainable and Futuristic Technologies (ICSFT 2026)

## ğŸ”® Future Work
- Multi-task reliability learning
- Attention-based introspection
- Active learning with reliability feedback
- Extension to other modalities (medical imaging, surveillance)

## ğŸ“„ License
This project is intended for academic and research use.
